{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e1YwuFtZd1t",
   "metadata": {
    "editable": true,
    "id": "1e1YwuFtZd1t",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 第8章: ニューラルネット\n",
    "\n",
    "第7章で取り組んだポジネガ分類を題材として、ニューラルネットワークで分類モデルを実装する。なお、この章ではPyTorchやTensorFlow、JAXなどの深層学習フレームワークを活用せよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03603ee-a54b-4b93-97a2-888f5e3feeff",
   "metadata": {
    "id": "b03603ee-a54b-4b93-97a2-888f5e3feeff"
   },
   "source": [
    "## 70. 単語埋め込みの読み込み\n",
    "\n",
    "事前学習済み単語埋め込みを活用し、$|V| \\times d_\\mathrm{emb}$ の単語埋め込み行列$\\pmb{E}$を作成せよ。ここで、$|V|$は単語埋め込みの語彙数、$d_\\mathrm{emb}$は単語埋め込みの次元数である。ただし、単語埋め込み行列の先頭の行ベクトル$\\pmb{E}_{0,:}$は、将来的にパディング（`<PAD>`）トークンの埋め込みベクトルとして用いたいので、ゼロベクトルとして予約せよ。ゆえに、$\\pmb{E}$の2行目以降に事前学習済み単語埋め込みを読み込むことになる。\n",
    "\n",
    "もし、Google Newsデータセットの[学習済み単語ベクトル](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)（300万単語・フレーズ、300次元）を全て読み込んだ場合、$|V|=3000001, d_\\mathrm{emb}=300$になるはずである（ただ、300万単語の中には、殆ど用いられない稀な単語も含まれるので、語彙を削減した方がメモリの節約になる）。\n",
    "\n",
    "また、単語埋め込み行列の構築と同時に、単語埋め込み行列の各行のインデックス番号（トークンID）と、単語（トークン）への双方向の対応付けを保持せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04038bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000001, 300)\n",
      "|V|:3000001\n",
      "d_emb:300\n"
     ]
    }
   ],
   "source": [
    "#第6章のデータを使用した．（Google Newsデータセット）\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "model_path = os.getenv(\"GoogleNews_Vector\")\n",
    "model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "\n",
    "vocab_size = len(model.key_to_index)\n",
    "emb_dim = model.vector_size\n",
    "\n",
    "word_to_id = {\"<PAD>\":0}\n",
    "id_to_word = {0:\"<PAD>\"}\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size+1, emb_dim))\n",
    "\n",
    "for word, gensim_id in model.key_to_index.items():\n",
    "    word_id = gensim_id + 1\n",
    "    word_to_id[word] = word_id\n",
    "    id_to_word[word_id] = word\n",
    "    embedding_matrix[word_id] = model[word]\n",
    "    \n",
    "print(embedding_matrix.shape)\n",
    "print(f\"|V|:{embedding_matrix.shape[0]}\")\n",
    "print(f\"d_emb:{embedding_matrix.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45bc5ba-4a83-493a-a78e-04aa48f3db2e",
   "metadata": {
    "id": "c45bc5ba-4a83-493a-a78e-04aa48f3db2e"
   },
   "source": [
    "## 71. データセットの読み込み\n",
    "\n",
    "[General Language Understanding Evaluation (GLUE)](https://gluebenchmark.com/) ベンチマークで配布されている[Stanford Sentiment Treebank (SST)](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip) をダウンロードし、訓練セット（train.tsv）と開発セット（dev.tsv）のテキストと極性ラベルと読み込み、全てのテキストをトークンID列に変換せよ。このとき、単語埋め込みの語彙でカバーされていない単語は無視し、トークン列に含めないことにせよ。また、テキストの全トークンが単語埋め込みの語彙に含まれておらず、空のトークン列となってしまう事例は、訓練セットおよび開発セットから削除せよ（このため、第7章の実験で得られた正解率と比較できなくなることに注意せよ）。\n",
    "\n",
    "事例の表現方法は任意でよいが、例えば\"contains no wit , only labored gags\"がネガティブに分類される事例は、次のような辞書オブジェクトで表現すればよい。\n",
    "\n",
    "```\n",
    "{'text': 'contains no wit , only labored gags',\n",
    " 'label': tensor([0.]),\n",
    " 'input_ids': tensor([ 3475,    87, 15888,    90, 27695, 42637])}\n",
    "```\n",
    "\n",
    "この例では、`text`はテキスト、`label`は分類ラベル（ポジティブなら`tensor([1.])`、ネガティブなら`tensor([0.])`）、`input_ids`はテキストのトークン列をID列で表現している。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "641059cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'hide new secretions from the parental units ', 'label': tensor([0.]), 'input_ids': tensor([  5785,     66, 113845,     18,     12,  15095,   1594])}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import torch\n",
    "\n",
    "def load_sst2(file_path, word_to_id):\n",
    "    data = []\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f, delimiter=\"\\t\")\n",
    "        for row in reader:\n",
    "            text = row[\"sentence\"]\n",
    "            label = float(row[\"label\"])\n",
    "\n",
    "            words = text.split()\n",
    "            input_ids = [word_to_id[w] for w in words if w in word_to_id]\n",
    "\n",
    "            if len(input_ids) == 0:\n",
    "                continue\n",
    "\n",
    "            data.append({\n",
    "                \"text\": text,\n",
    "                \"label\": torch.tensor([label]),\n",
    "                \"input_ids\": torch.tensor(input_ids)\n",
    "            })\n",
    "    return data\n",
    "\n",
    "train_path = \"../ch07/data/SST-2/train.tsv\"\n",
    "dev_path= \"../ch07/data/SST-2/dev.tsv\"\n",
    "\n",
    "train_data = load_sst2(train_path, word_to_id)\n",
    "dev_data = load_sst2(dev_path, word_to_id)\n",
    "\n",
    "# print(f\"train_data: {len(train_data)}\")\n",
    "# print(f\"dev_data: {len(dev_data)}\")\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dfe527-a08c-48fa-b9b4-0acebea36bca",
   "metadata": {
    "id": "29dfe527-a08c-48fa-b9b4-0acebea36bca"
   },
   "source": [
    "## 72. Bag of wordsモデルの構築\n",
    "\n",
    "単語埋め込みの平均ベクトルでテキストの特徴ベクトルを表現し、重みベクトルとの内積でポジティブ及びネガティブを分類するニューラルネットワーク（ロジスティック回帰モデル）を設計せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9742d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class AvgvecEmbclassification(nn.Module):\n",
    "    def __init__(self, embedding):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding,freeze=True,padding_idx=0)\n",
    "        self.linear = nn.Linear(embedding.size(1), 1)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embeds = self.embedding(input_ids)   \n",
    "        sent_vec = embeds.mean(dim=0)         \n",
    "        logit = self.linear(sent_vec)         \n",
    "        prob = torch.sigmoid(logit)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72385c44-ceab-4d62-a4df-3023e15a37e2",
   "metadata": {
    "id": "72385c44-ceab-4d62-a4df-3023e15a37e2"
   },
   "source": [
    "## 73. モデルの学習\n",
    "\n",
    "問題72で設計したモデルの重みベクトルを訓練セット上で学習せよ。ただし、学習中は単語埋め込み行列の値を固定せよ（単語埋め込み行列のファインチューニングは行わない）。また、学習時に損失値を表示するなど、学習の進捗状況をモニタリングできるようにせよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3a07b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 0.4022, Dev Accuracy: 0.7936\n",
      "Epoch 2/5, Train Loss: 0.3725, Dev Accuracy: 0.8005\n",
      "Epoch 3/5, Train Loss: 0.3701, Dev Accuracy: 0.7982\n",
      "Epoch 4/5, Train Loss: 0.3692, Dev Accuracy: 0.7993\n",
      "Epoch 5/5, Train Loss: 0.3688, Dev Accuracy: 0.7993\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "embedding_tensor = torch.FloatTensor(embedding_matrix)\n",
    "model = AvgvecEmbclassification(embedding_tensor).to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.linear.parameters(), lr=1e-3)\n",
    "\n",
    "def train_eval(model,data):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for sample in data:\n",
    "        input_ids = sample[\"input_ids\"].to(device)\n",
    "        label = sample[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        prob = model(input_ids)\n",
    "        loss = criterion(prob, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    avg_loss = total_loss / len(data)\n",
    "    return avg_loss\n",
    "\n",
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for sample in data:\n",
    "            input_ids = sample[\"input_ids\"].to(device)\n",
    "            label = sample[\"label\"].to(device)\n",
    "\n",
    "            prob = model(input_ids)\n",
    "            pred = (prob >= 0.5).float()\n",
    "            if pred.item() == label.item():\n",
    "                correct += 1\n",
    "    accuracy = correct / len(data)\n",
    "    return accuracy\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_eval(model, train_data)\n",
    "    dev_acc = evaluate(model, dev_data)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Dev Accuracy: {dev_acc:.4f}\")\n",
    "    \n",
    "    if dev_acc > best_acc:\n",
    "        best_acc = dev_acc\n",
    "        torch.save(model.state_dict(), \"../ch08/model/73_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b25b5b-0ed2-4bf0-8350-601812eb057f",
   "metadata": {
    "id": "26b25b5b-0ed2-4bf0-8350-601812eb057f"
   },
   "source": [
    "## 74. モデルの評価\n",
    "\n",
    "問題73で学習したモデルの開発セットにおける正解率を求めよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49ef936a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_dev_acc: 0.8005\n"
     ]
    }
   ],
   "source": [
    "model = AvgvecEmbclassification(embedding_tensor).to(device)\n",
    "model.load_state_dict(torch.load(\"../ch08/model/73_model.pt\",weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "dev_acc = evaluate(model, dev_data)\n",
    "print(f\"best_dev_acc: {dev_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867da822",
   "metadata": {},
   "source": [
    "・推奨\n",
    "/tmp/ipykernel_596915/2353182659.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
    "  model.load_state_dict(torch.load(\"../ch08/model/73_model.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O08V9g0mcJwe",
   "metadata": {
    "id": "O08V9g0mcJwe"
   },
   "source": [
    "## 75. パディング\n",
    "\n",
    "複数の事例が与えられたとき、これらをまとめて一つのテンソル・オブジェクトで表現する関数`collate`を実装せよ。与えられた複数の事例のトークン列の長さが異なるときは、トークン列の長さが最も長いものに揃え、0番のトークンIDでパディングをせよ。さらに、トークン列の長さが長いものから順に、事例を並び替えよ。\n",
    "\n",
    "例えば、訓練データセットの冒頭の4事例が次のように表されているとき、\n",
    "\n",
    "```\n",
    "[{'text': 'hide new secretions from the parental units',\n",
    "  'label': tensor([0.]),\n",
    "  'input_ids': tensor([  5785,     66, 113845,     18,     12,  15095,   1594])},\n",
    " {'text': 'contains no wit , only labored gags',\n",
    "  'label': tensor([0.]),\n",
    "  'input_ids': tensor([ 3475,    87, 15888,    90, 27695, 42637])},\n",
    " {'text': 'that loves its characters and communicates something rather beautiful about human nature',\n",
    "  'label': tensor([1.]),\n",
    "  'input_ids': tensor([    4,  5053,    45,  3305, 31647,   348,   904,  2815,    47,  1276,  1964])},\n",
    " {'text': 'remains utterly satisfied to remain the same throughout',\n",
    "  'label': tensor([0.]),\n",
    "  'input_ids': tensor([  987, 14528,  4941,   873,    12,   208,   898])}]\n",
    "```\n",
    "\n",
    "`collate`関数を通した結果は以下のようになることが想定される。\n",
    "\n",
    "```\n",
    "{'input_ids': tensor([\n",
    "    [     4,   5053,     45,   3305,  31647,    348,    904,   2815,     47,   1276,   1964],\n",
    "    [  5785,     66, 113845,     18,     12,  15095,   1594,      0,      0,      0,      0],\n",
    "    [   987,  14528,   4941,    873,     12,    208,    898,      0,      0,      0,      0],\n",
    "    [  3475,     87,  15888,     90,  27695,  42637,      0,      0,      0,      0,      0]]),\n",
    " 'label': tensor([\n",
    "    [1.],\n",
    "    [0.],\n",
    "    [0.],\n",
    "    [0.]])}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "659ef3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    batch = sorted(batch, key=lambda x:len(x[\"input_ids\"]),reverse=True)\n",
    "    max_len=len(batch[0][\"input_ids\"])\n",
    "    \n",
    "    padded_ids = []\n",
    "    labels = []\n",
    "    \n",
    "    for sample in batch:\n",
    "        ids = sample[\"input_ids\"]\n",
    "        padding_len = max_len - len(ids)\n",
    "        \n",
    "        padding = torch.cat([ids, torch.zeros(padding_len,dtype=torch.long)])\n",
    "        padded_ids.append(padding.unsqueeze(0))\n",
    "        labels.append(sample[\"label\"].unsqueeze(0))\n",
    "        result  = {\"input_ids\": torch.cat(padded_ids,dim=0),\"labels\": torch.cat(labels,dim=0)}\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9NzvuZ-5ebDU",
   "metadata": {
    "id": "9NzvuZ-5ebDU"
   },
   "source": [
    "## 76. ミニバッチ学習\n",
    "\n",
    "問題75のパディングの処理を活用して、ミニバッチでモデルを学習せよ。また、学習したモデルの開発セットにおける正解率を求めよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35bc02ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6647, Dev Accuracy: 0.6786\n",
      "Epoch 2/10, Train Loss: 0.6178, Dev Accuracy: 0.7500\n",
      "Epoch 3/10, Train Loss: 0.5873, Dev Accuracy: 0.7143\n",
      "Epoch 4/10, Train Loss: 0.5682, Dev Accuracy: 0.8214\n",
      "Epoch 5/10, Train Loss: 0.5440, Dev Accuracy: 0.8214\n",
      "Epoch 6/10, Train Loss: 0.5257, Dev Accuracy: 0.8214\n",
      "Epoch 7/10, Train Loss: 0.5173, Dev Accuracy: 0.8214\n",
      "Epoch 8/10, Train Loss: 0.5139, Dev Accuracy: 0.8214\n",
      "Epoch 9/10, Train Loss: 0.4940, Dev Accuracy: 0.8214\n",
      "Epoch 10/10, Train Loss: 0.4920, Dev Accuracy: 0.8214\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,collate_fn=collate)\n",
    "dev_loader = DataLoader(dev_data,batch_size=batch_size,shuffle=False,collate_fn=collate)\n",
    "\n",
    "class AvgvecEmbclassification_batch(nn.Module):\n",
    "    def __init__(self, embedding):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding,freeze=True,padding_idx=0)\n",
    "        self.linear = nn.Linear(embedding.size(1), 1)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embeds = self.embedding(input_ids)   \n",
    "        sent_vec = embeds.mean(dim=1)         \n",
    "        logit = self.linear(sent_vec)         \n",
    "        prob = torch.sigmoid(logit)\n",
    "        return prob\n",
    "\n",
    "def train_one_epoch(model, data_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        prob = model(input_ids)\n",
    "        loss = criterion(prob, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def evaluate(model,loader,device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            prob = model(input_ids)\n",
    "            preds = (prob >= 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "    return correct / total\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "embedding_tenser = torch.FloatTensor(embedding_matrix)\n",
    "model = AvgvecEmbclassification_batch(embedding_tenser).to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.linear.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    dev_acc = evaluate(model, dev_loader, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Dev Accuracy: {dev_acc:.4f}\")\n",
    "\n",
    "    if dev_acc > best_acc:\n",
    "        best_acc = dev_acc\n",
    "        torch.save(model.state_dict(), \"../ch08/model/76_model.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RUbjivUTejxn",
   "metadata": {
    "id": "RUbjivUTejxn"
   },
   "source": [
    "## 77. GPU上での学習\n",
    "\n",
    "問題76のモデル学習をGPU上で実行せよ。また、学習したモデルの開発セットにおける正解率を求めよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f751956",
   "metadata": {},
   "source": [
    "76.と同様．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZUY1PsD-eplq",
   "metadata": {
    "id": "ZUY1PsD-eplq"
   },
   "source": [
    "## 78. 単語埋め込みのファインチューニング\n",
    "\n",
    "問題77の学習において、単語埋め込みのパラメータも同時に更新するファインチューニングを導入せよ。また、学習したモデルの開発セットにおける正解率を求めよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c42137",
   "metadata": {},
   "source": [
    "76の問題との主な違い：  \n",
    "・embeddingを\"freeze=True\"から\"freeze=False\"に変更する．  \n",
    "・optimizerにmodel.parameters()を渡す．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42a4dfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 0.5359, Dev Accuracy: 0.7143\n",
      "Epoch 2/5, Train Loss: 0.2744, Dev Accuracy: 0.7143\n",
      "Epoch 3/5, Train Loss: 0.1917, Dev Accuracy: 0.7500\n",
      "Epoch 4/5, Train Loss: 0.1707, Dev Accuracy: 0.7857\n",
      "Epoch 5/5, Train Loss: 0.1322, Dev Accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,collate_fn=collate)\n",
    "dev_loader = DataLoader(dev_data,batch_size=batch_size,shuffle=False,collate_fn=collate)\n",
    "\n",
    "class AvgvecEmbclassification_batch(nn.Module):\n",
    "    def __init__(self, embedding):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding,freeze=False,padding_idx=0)\n",
    "        self.linear = nn.Linear(embedding.size(1), 1)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embeds = self.embedding(input_ids)   \n",
    "        sent_vec = embeds.mean(dim=1)         \n",
    "        logit = self.linear(sent_vec)         \n",
    "        prob = torch.sigmoid(logit)\n",
    "        return prob\n",
    "\n",
    "def train_one_epoch(model, data_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        prob = model(input_ids)\n",
    "        loss = criterion(prob, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def evaluate(model,loader,device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            prob = model(input_ids)\n",
    "            preds = (prob >= 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "    return correct / total\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "embedding_tenser = torch.FloatTensor(embedding_matrix)\n",
    "model = AvgvecEmbclassification_batch(embedding_tenser).to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    dev_acc = evaluate(model, dev_loader, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Dev Accuracy: {dev_acc:.4f}\")\n",
    "\n",
    "    if dev_acc > best_acc:\n",
    "        best_acc = dev_acc\n",
    "        torch.save(model.state_dict(), \"../ch08/model/78_model.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jVAdWIq0evKR",
   "metadata": {
    "id": "jVAdWIq0evKR"
   },
   "source": [
    "## 79. アーキテクチャの変更\n",
    "\n",
    "ニューラルネットワークのアーキテクチャを自由に変更し、モデルを学習せよ。また、学習したモデルの開発セットにおける正解率を求めよ。例えば、テキストの特徴ベクトル（単語埋め込みの平均ベクトル）に対して多層のニューラルネットワークを通したり、畳み込みニューラルネットワーク（CNN; Convolutional Neural Network）や再帰型ニューラルネットワーク（RNN; Recurrent Neural Network）などのモデルの学習に挑戦するとよい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e307a3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 0.3916, Dev Accuracy: 0.8000\n",
      "Epoch 2/5, Train Loss: 0.2074, Dev Accuracy: 0.7455\n",
      "Epoch 3/5, Train Loss: 0.1743, Dev Accuracy: 0.7818\n",
      "Epoch 4/5, Train Loss: 0.1423, Dev Accuracy: 0.6909\n",
      "Epoch 5/5, Train Loss: 0.1341, Dev Accuracy: 0.7091\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, embedding, num_filters=100, kernel_size=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding,freeze=False,padding_idx=0)\n",
    "        \n",
    "        self.conv = nn.Conv1d(in_channels=embedding.size(1), out_channels=num_filters, kernel_size=kernel_size)\n",
    "        self.fc = nn.Linear(num_filters, 1)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        embeds = self.embedding(input_ids)\n",
    "        x = embeds.transpose(1, 2)\n",
    "        x = torch.relu(self.conv(x))\n",
    "        x = torch.max(x, dim=2).values\n",
    "        logit = self.fc(x)\n",
    "        prob = torch.sigmoid(logit)\n",
    "        return prob\n",
    "   \n",
    "    \n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,collate_fn=collate)\n",
    "dev_loader = DataLoader(dev_data,batch_size=batch_size,shuffle=False,collate_fn=collate)\n",
    "\n",
    "def train_one_epoch(model, data_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        prob = model(input_ids)\n",
    "        loss = criterion(prob, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def evaluate(model,loader,device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            prob = model(input_ids)\n",
    "            preds = (prob >= 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "    return correct / total\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "embedding_tenser = torch.FloatTensor(embedding_matrix)\n",
    "model = CNN(embedding_tenser).to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    dev_acc = evaluate(model, dev_loader, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Dev Accuracy: {dev_acc:.4f}\")\n",
    "\n",
    "    if dev_acc > best_acc:\n",
    "        best_acc = dev_acc\n",
    "        torch.save(model.state_dict(), \"../ch08/model/79_model.pt\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp100",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
