{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b41f8ff5",
   "metadata": {},
   "source": [
    "## 00. パタトクカシーー\n",
    "\n",
    "2つの文字列「パトカー」と「タクシー」の文字を先頭から交互に連結し，文字列「パタトクカシーー」を得よ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f31e6a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "パタトクカシーー\n"
     ]
    }
   ],
   "source": [
    "text1 = \"パトカー\"\n",
    "text2 = \"タクシー\"\n",
    "con_text = \"\".join([a+b for a,b in zip(text1, text2)])\n",
    "print(con_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca48c798",
   "metadata": {},
   "source": [
    "## 01. タクシー\n",
    "\n",
    "文字列「パタトクカシーー」の2, 4, 6, 8文字目を取り出し，それらを連結した文字列を得よ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a917ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "タクシー\n"
     ]
    }
   ],
   "source": [
    "text = \"パタトクカシーー\"\n",
    "select_text = \"\".join(text[i] for i in range(1,len(text),2))\n",
    "print(select_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe60151",
   "metadata": {},
   "source": [
    "## 02. 文字列の逆順\n",
    "\n",
    "文字列”stressed”の文字を逆に（末尾から先頭に向かって）並べた文字列を得よ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb63534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desserts\n"
     ]
    }
   ],
   "source": [
    "text=\"stressed\"\n",
    "rev_tex=text[::-1]\n",
    "print(rev_tex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a9668",
   "metadata": {},
   "source": [
    "## 03. 円周率\n",
    "\n",
    "“Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.”という文を単語に分解し，各単語の（アルファベットの）文字数を先頭から出現順に並べたリストを作成せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b48f11a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8, 9, 7, 9]\n"
     ]
    }
   ],
   "source": [
    "text = \"Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.\"\n",
    "words = text.replace(\",\",\"\").replace(\".\",\"\").split()\n",
    "words_len=[len(word) for word in words]\n",
    "print(words_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b2af8c",
   "metadata": {},
   "source": [
    "## 04. 元素記号\n",
    "\n",
    "“Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.”という文を単語に分解し，1, 5, 6, 7, 8, 9, 15, 16, 19番目の単語は先頭の1文字，それ以外の単語は先頭の2文字を取り出し，取り出した文字列から単語の位置（先頭から何番目の単語か）への連想配列（辞書型もしくはマップ型）を作成せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a01c9f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'H', 2: 'He', 3: 'Li', 4: 'Be', 5: 'B', 6: 'C', 7: 'N', 8: 'O', 9: 'F', 10: 'Ne', 11: 'Na', 12: 'Mi', 13: 'Al', 14: 'Si', 15: 'P', 16: 'S', 17: 'Cl', 18: 'Ar', 19: 'K', 20: 'Ca'}\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.\"\n",
    "words = sentence.replace(\".\",\"\").split()\n",
    "select=[1, 5, 6, 7, 8, 9, 15, 16, 19]\n",
    "show = {i+1:(word[:1] if i+1 in select else word[:2]) for i,word in enumerate(words)}\n",
    "print(show)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffc8d52",
   "metadata": {},
   "source": [
    "## 05. n-gram\n",
    "\n",
    "与えられたシーケンス（文字列やリストなど）からn-gramを作る関数を作成せよ．この関数を用い，”I am an NLPer”という文から文字tri-gram，単語bi-gramを得よ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f9142fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ia', 'am', 'ma', 'an', 'nN', 'NL', 'LP', 'Pe', 'er']\n",
      "[['I', 'am'], ['am', 'an'], ['an', 'NLPer']]\n"
     ]
    }
   ],
   "source": [
    "def n_gram(sequence ,n):\n",
    "  return [sequence[i:i+n] for i in range(len(sequence)-n+1)]\n",
    "\n",
    "text = \"I am an NLPer\"\n",
    "\n",
    "word1 = text.replace(\" \",\"\")\n",
    "word2 = text.split()\n",
    "\n",
    "print(n_gram(word1,2))\n",
    "print(n_gram(word2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1e75f7",
   "metadata": {},
   "source": [
    "## 06. 集合\n",
    "\n",
    "“paraparaparadise”と”paragraph”に含まれる文字bi-gramの集合を,それぞれ,XとYとして求め，XとYの和集合，積集合，差集合を求めよ．さらに，'se'というbi-gramがXおよびYに含まれるかどうかを調べよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d50c7fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "union: {'pa', 'se', 'ph', 'ag', 'ap', 'di', 'ar', 'is', 'ad', 'ra', 'gr'}\n",
      "intresection: {'ar', 'pa', 'ap', 'ra'}\n",
      "difference: {'is', 'di', 'se', 'ad'}\n",
      "answer: se is in X\n"
     ]
    }
   ],
   "source": [
    "text1= \"paraparaparadise\"\n",
    "text2=\"paragraph\"\n",
    "text1=n_gram(text1,2)\n",
    "text2=n_gram(text2,2)\n",
    "\n",
    "X = set(text1)\n",
    "Y = set(text2)\n",
    "\n",
    "print(\"union:\",X | Y)\n",
    "print(\"intresection:\", X & Y)\n",
    "print(\"difference:\", X - Y)\n",
    "\n",
    "print(\"answer: \",end =\"\")\n",
    "search = 'se'\n",
    "if search in X and search in Y:\n",
    "  print(f\"{search} is in noth X and Y\")\n",
    "elif search in X:\n",
    "  print(f\"{search} is in X\")\n",
    "elif search in Y:\n",
    "  print(f\"{search} is in Y\")\n",
    "else:\n",
    "  print(f\"{search} is not in X and Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ba003c",
   "metadata": {},
   "source": [
    "## 07. テンプレートによる文生成\n",
    "\n",
    "引数x, y, zを受け取り「x時のyはz」という文字列を返す関数を実装せよ．さらに，x=12, y=”気温”, z=22.4として，実行結果を確認せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f82ea0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12時の気温は22.4\n"
     ]
    }
   ],
   "source": [
    "def time_details(x,y,z):\n",
    "  tmp = f'{x}時の{y}は{z}'\n",
    "  return tmp\n",
    "\n",
    "print(time_details(12,\"気温\",22.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7cdf80",
   "metadata": {},
   "source": [
    "## 08. 暗号化\n",
    "与えられた文字列の各文字を，以下の仕様で変換する関数cipherを実装せよ．\n",
    "\n",
    "・英小文字ならば (219 - 文字コード) のASCIIコードに対応する文字に置換  \n",
    "・その他の文字はそのまま出力\n",
    "\n",
    "この関数を用い，英語のメッセージを暗号化・復号化せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7ccfa20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nzgfizo Lzmtfztv Pilxvhhrmt Lzylizglib\n"
     ]
    }
   ],
   "source": [
    "def cipher(text):\n",
    "  result = \"\"\n",
    "  for char in text:\n",
    "    if char.islower():\n",
    "      result += chr(219-ord(char))\n",
    "    else:\n",
    "      result += char\n",
    "\n",
    "  return result\n",
    "\n",
    "text = \"Natural Language Processing Laboratory\"\n",
    "print(cipher(text)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068b282e",
   "metadata": {},
   "source": [
    "## 09. Typoglycemia\n",
    "\n",
    "スペースで区切られた単語列に対して，各単語の先頭と末尾の文字は残し，それ以外の文字の順序をランダムに並び替えるプログラムを作成せよ．ただ，長さが４以下の単語は並び替えないこととする．適当な英語の文（例えば”I couldn’t believe that I could actually understand what I was reading : the phenomenal power of the human mind .”）を与え，その実行結果を確認せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "616c0e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cd'lunot beleive that I culod aatcluly utradnesnd what I was reinadg the pnaehmenol power of the hamun mind\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def typoglycemia(text):\n",
    "  words = text.replace(\",\",\"\").replace(\".\",\"\").replace(\":\",\"\").split()\n",
    "  processed = []\n",
    "  for word in words:\n",
    "    if len(word) <= 4:\n",
    "      processed.append(word)\n",
    "    else:\n",
    "        middle = (word[1:-1])\n",
    "        processed.append(word[0] + \"\".join(random.sample(middle,len(middle))) + word[-1])\n",
    "\n",
    "  return \" \".join(processed)\n",
    "\n",
    "text = \"I couldn't believe that I could actually understand what I was reading : the phenomenal power of the human mind .\"\n",
    "result = typoglycemia(text)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp100",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
